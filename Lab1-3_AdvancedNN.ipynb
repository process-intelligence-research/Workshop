{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3209aede",
   "metadata": {},
   "source": [
    "# **Preface**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23418405",
   "metadata": {},
   "source": [
    "Before you start working on the tutorial, please ensure you complete the following preparatory steps:\n",
    "\n",
    "- Install Python version 3.11.7. You can download it from the official Python website: https://www.python.org/downloads/release/python-3117/.\n",
    "\n",
    "- Obtain the `Venv Setup Tutorial.pdf` and `requirements.txt` files, also available in the same section: BrightSpace -> Content -> Week 1 -> Extra materials.\n",
    "\n",
    "- Follow the instructions detailed in the `Venv Setup Tutorial.pdf` to set up your virtual environment.\n",
    "\n",
    "These steps are crucial for ensuring that your working environment is correctly configured for the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f81527",
   "metadata": {
    "id": "64f81527"
   },
   "source": [
    "# Tutorial 3\n",
    "\n",
    "In the second lecture you were introduced to the applications of NNs for regression and classification purposes. This tutorial has been split into two sections. The first one focusses on applications of neural networks for regression purposes, the assessment of the goodness-of-fit of the model being developed, and its extrapolating capabilities. In the second part, you will work on a QSPR analysis by using neural networks.\n",
    "\n",
    "This tutorial covers the following topics:\n",
    "\n",
    "* 1D regression\n",
    "* Assessment of the extrapolation capabilities of the regression\n",
    "* QSPR method using NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3814cac",
   "metadata": {
    "id": "f3814cac"
   },
   "source": [
    "## Regression with artificial neural networks in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0b4b5",
   "metadata": {
    "id": "29e0b4b5"
   },
   "source": [
    "An artificial neural network (ANN) is an universal function approximator. Contrary to curve-fitting regression, ANNs have the advantage that no a priori knowledge of the process is needed. Here we show an example of a simple ANN to approximate its target distribution. For the demonstration we use the programming language Python since it is the most popular machine learning language and offers a large and active developer community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42413d7a",
   "metadata": {
    "id": "42413d7a"
   },
   "source": [
    "Here we use [PyTorch](https://pytorch.org/) to work with neural networks. Pytorch is an open source machine learning framework with many predefined functions which make the work with machine learning way easier. You can learn pytorch from the [tutorial link](https://pytorch.org/tutorials/). The Documentation can be found at [Docs](https://pytorch.org/docs/stable/index.html). A popular alternative to Pytorch is Keras (building on Tensorflow). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935629e",
   "metadata": {
    "id": "1935629e"
   },
   "source": [
    "## 1.1 - Python preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d055ad",
   "metadata": {
    "id": "46d055ad"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230200b4",
   "metadata": {
    "id": "230200b4"
   },
   "source": [
    "In the next step, we import the previously installed packages into our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d254a",
   "metadata": {
    "id": "067d254a"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # numerical calculations in python\n",
    "import matplotlib.pyplot as plt  # plotting similar to matlab\n",
    "import torch  # PyTorch: the general machine learning framework in Python\n",
    "import torch.optim as optim  # contains optimizers for the backpropagation\n",
    "import torch.nn as nn  # the artificial neural network module in PyTorch\n",
    "from tqdm import tqdm  # produces progress bars for for-loops in Python\n",
    "from sklearn.model_selection import train_test_split  # randomly splits a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5f975",
   "metadata": {
    "id": "87a5f975"
   },
   "source": [
    "### Set the seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582024ce",
   "metadata": {
    "id": "582024ce"
   },
   "source": [
    "To make our results reproducible, we need to set a so-called \"seed\". Machine Learning includes stochastic processes in the weight/bias initialization and the backpropagation. Also the random number generation which we will use for the dataset is a stochastic process. By setting a seed in the program we make sure that always the same random numbers are chosen. Otherwise, we would get different results everytime we run this script, which is not nice for teaching purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345c218",
   "metadata": {
    "id": "4345c218"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2823b95",
   "metadata": {
    "id": "a2823b95"
   },
   "source": [
    "## 1.2 - Prepare datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdef9fa",
   "metadata": {
    "id": "4fdef9fa"
   },
   "source": [
    "First and foremost, we need a dataset to work on. Here, we simply make up our dataset from a self defined model.\n",
    "\n",
    "\n",
    "Now we define the model which we want to approximate. We add the option to add some noise to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa4154",
   "metadata": {
    "id": "f5aa4154"
   },
   "outputs": [],
   "source": [
    "# Defining the model we want to approximate\n",
    "def model(x, noise=False):\n",
    "    y = np.sin(x)+np.sin(10/3*x) # model function\n",
    "    if noise:\n",
    "        y += 0.3*(np.random.uniform(-1, 1, x.size)) # add noise if noise = True\n",
    "    return y # return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614482e",
   "metadata": {
    "id": "8614482e"
   },
   "source": [
    "We now use our model to generate a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c91a8d",
   "metadata": {
    "id": "35c91a8d"
   },
   "outputs": [],
   "source": [
    "n = 100 # number of data points\n",
    "xmin = 2 # minimum value\n",
    "xmax = 6 # maximum value\n",
    "x = np.linspace(xmin, xmax, n) # generate equally spaced input values\n",
    "y = model(x, noise=True) # get output from our model with noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5788fe",
   "metadata": {
    "id": "ac5788fe"
   },
   "source": [
    "For a better understanding of the data set we plot it with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfdd618",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "7cfdd618",
    "outputId": "f0a7c506-aa46-4b25-e10f-ee2afc89b975"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4)) # define figure size\n",
    "plt.plot(x, model(x), color='r', linestyle='--', label='ground truth')\n",
    "plt.scatter(x, y, alpha=0.5, color='b', label='complete data') # scattered plot\n",
    "plt.title('Initial dataset') # add plot title\n",
    "plt.xlabel('x') # add x axis label\n",
    "plt.ylabel('y') # add y axis label\n",
    "plt.legend() # add plot legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c87d50",
   "metadata": {
    "id": "47c87d50"
   },
   "source": [
    "We need to split the dataset into training, validation and test sets. We initially divide the data into a training set and a remaining dataset, with train_size=0.8 to ensure that 80% of the data is in the training set. In order to ensure that both the validation and test sets are of equal size, constituting 10% each of the overall data, we specify test_size=0.5 which is equivalent to 50% of the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae436d96",
   "metadata": {
    "id": "ae436d96"
   },
   "outputs": [],
   "source": [
    "# Placeholder variables for the training, validation and test sets\n",
    "x_train, x_val, x_test = None, None, None\n",
    "y_train, y_val, y_test = None, None, None\n",
    "\n",
    "###################################################################################################\n",
    "# TODO: Split the data into training, validation and test sets.                                   #\n",
    "# Helpful Link:                                                                                   #\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html #\n",
    "# 1. Split the data into training set and remaining data, set the random_state to 2024.           #\n",
    "# 2. Split the remaining data into validation and test set.                                       #\n",
    "###################################################################################################\n",
    "\n",
    "# Replace the pass statement with your code\n",
    "pass\n",
    "\n",
    "###################################################################################################\n",
    "#                             END OF YOUR CODE                                                    #\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0807f",
   "metadata": {
    "id": "95d0807f"
   },
   "source": [
    "We also plot the dataset division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130fa6ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "130fa6ff",
    "outputId": "f4a01448-04ae-4279-905d-823ee97ae9f1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4)) # define figure size\n",
    "plt.plot(x, model(x), color='r', linestyle='--', label='ground truth') # line plot\n",
    "plt.scatter(x_train, y_train, alpha=0.5, label='training data') # scattered plot\n",
    "plt.scatter(x_val, y_val, alpha=0.5, label='validation data') # scattered plot\n",
    "plt.scatter(x_test, y_test, alpha=0.5, label='test data') # scattered plot\n",
    "plt.title('Dataset division') # add plot title\n",
    "plt.xlabel('x') # add x axis label\n",
    "plt.ylabel('y') # add y axis label\n",
    "plt.legend() # add plot legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa7a9c",
   "metadata": {
    "id": "57fa7a9c"
   },
   "source": [
    "So far our datasets are stored in numpy arrays. However, PyTorch works with tensors instead of arrays and we need to transform our data. [Here](https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c) you can find a blog post discussing the differences. We need to do two small technical changes as well. Numpy arrays usually use the double/float64 datatype whereas PyTorch uses float/float32. Therefore, we change the datatype *dtype*. In addition, we have to change the shape of the tensor from (#data) to (#data,1) using *unsqueeze*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782ac00",
   "metadata": {
    "id": "a782ac00"
   },
   "outputs": [],
   "source": [
    "xt_train = torch.tensor(x_train, dtype=torch.float).unsqueeze(-1)\n",
    "xt_val = torch.tensor(x_val, dtype=torch.float).unsqueeze(-1)\n",
    "xt_test = torch.tensor(x_test, dtype=torch.float).unsqueeze(-1)\n",
    "yt_train = torch.tensor(y_train, dtype=torch.float).unsqueeze(-1)\n",
    "yt_val = torch.tensor(y_val, dtype=torch.float).unsqueeze(-1)\n",
    "yt_test = torch.tensor(y_test, dtype=torch.float).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1e9abb",
   "metadata": {
    "id": "7e1e9abb"
   },
   "source": [
    "## 1.3 - Set up ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d3c1a5",
   "metadata": {
    "id": "e2d3c1a5"
   },
   "source": [
    "Besides the data we also need to prepare our ANN. The *nn.Module* is the standard class for an ANN in PyTorch. The abbreviation *nn* stands for neural network. We build a child class of it where we specify our desired model architecture. Pytorch uses a base model object and adds the layers and activations as other objects in a sequential manner. The first layer must get an input dimensions matching the data, whereas the following can deduce their input size from the previous layer. The output layer then must match the dimension of the target values. Each ANN class needs a *forward* function which defines, how a signal propagates through the network.\n",
    "\n",
    "You might recognize that the definition of the network is based on a *class*. Classes are a fundamental concepts in Object Oriented programming, and also apply to Python.\n",
    "\n",
    "In case you want to refresh or get to know about the basics of classes, check out the following link:\n",
    "https://www.geeksforgeeks.org/python-classes-and-objects/\n",
    "\n",
    "The following link gives a bsaic introduction to building a model class in pytorch:\n",
    "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html#define-the-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ee59b",
   "metadata": {
    "id": "882ee59b"
   },
   "outputs": [],
   "source": [
    "# Neural network definition\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.architecture = nn.Sequential(\n",
    "            # Sequential model definition: add up layers & activation functions\n",
    "            nn.Linear(in_features=n_input, out_features=n_hidden, bias=True),  # input layer\n",
    "            nn.Tanh(), # activation function\n",
    "            nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),  # hidden layer\n",
    "            nn.Tanh(), # activation function\n",
    "            nn.Linear(in_features=n_hidden, out_features=n_output, bias=True)   # output layer\n",
    "        )\n",
    "    def forward(self, input): # feed forward path\n",
    "        output = self.architecture(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29606c5",
   "metadata": {
    "id": "f29606c5"
   },
   "source": [
    "## 1.4 - Train ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90ed14",
   "metadata": {
    "id": "4a90ed14"
   },
   "source": [
    "Now the fun begins. We put the dataset and ANN architecture together to \"train\" our ANN.\n",
    "\n",
    "We use the training data to train our neural network. This process is nothing else then optimizing the weights and biases in our network. Before starting the training process, we need to define a few things:\n",
    "- *optimizer*: Here we use *SGD* which stands for [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). With the optimizer, we also need to define the learning rate *lr*. It determines how fast we adopt the weights and biases during the training. If it is too high, the learning becomes instable and the loss increases. If it is too low, we need to many epochs and we do not reach a satisfying precision.\n",
    "- *loss function*: This is the objective of our training/optimization. For continuous outputs as in our example, you usually use the mean squared error (MSE). For discrete outputs, another function is needed, like the cross entropy.\n",
    "- *epochs*: How often do we want to repeat the training with our dataset?\n",
    "\n",
    "Optimizing these parameters is called hyerparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3e1a8",
   "metadata": {
    "id": "0bf3e1a8"
   },
   "source": [
    "---\n",
    "**Adjust the following hyperparameters to find the optimal combination:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184ea65",
   "metadata": {
    "id": "0184ea65"
   },
   "outputs": [],
   "source": [
    "hidden_size = 32 # number of neurons in the hidden layer\n",
    "learning_rate = 0.09 # learning rate for the backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49760819",
   "metadata": {
    "id": "49760819"
   },
   "source": [
    "The following cell prepares everything for the training. Before, *let's reflect on how many input variables and outputs should your network handle.*\n",
    "* Thus, **create a simple sketch of the network structure.**\n",
    "* Then, **proceed with the following cell.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2fdbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8a2fdbe",
    "outputId": "92b4ba2a-eaf5-4388-9006-b13cd375bf0d"
   },
   "outputs": [],
   "source": [
    "# Neural network training\n",
    "net = NeuralNetwork(1, hidden_size, 1) # Create instance of neural network\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate) # Choose optimizer and learning rate\n",
    "loss_fun = nn.MSELoss() # Define loss function\n",
    "epochs = 5000 # Set number of epochs\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee169f31",
   "metadata": {
    "id": "ee169f31"
   },
   "source": [
    "This is the main part of using an ANN: the actual \"training\". We give an input to the network and see how the output differs from our expected output. The difference is used to calculate the loss. Then we update the weights and biases such that the loss will be smaller in the next epoch.\n",
    "\n",
    "In the training process we use two datasets: the **training** and the **validation** data. The training data are used to calculate the training loss, which is then used for the backpropagation and the network update. The validation data are used to detect overfitting. We just calculate the loss for the validation data, but do not use it for the backpropagation. If the training and the validation loss diverge, we know that the network updates do not generalize for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db295d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "3db295d6",
    "outputId": "19a8e12f-80f7-41bb-d14b-a271a96c3974"
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# train the network\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # Training\n",
    "    # Placeholder variables for the predictions on the training set\n",
    "    y_pred = None    \n",
    "\n",
    "    ####################################################################################\n",
    "    # TODO: Setup the training process of an ANN.                                      # \n",
    "    # Helpful Link:                                                                    #\n",
    "    # https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop #\n",
    "    # 1. Clear the gradients for the next training epoch.                              #         \n",
    "    # 2. Compute the value of the predictions y for the training set.                  #\n",
    "    # 3. Calculate the loss between the predictions and the true values.               #\n",
    "    # 4. Backward pass, compute gradients.                                             #\n",
    "    # 5. Apply gradients to update weights and biases.                                 #\n",
    "    ####################################################################################\n",
    "    \n",
    "    # Replace the pass statement with your code\n",
    "    pass\n",
    "\n",
    "    ######################################################################\n",
    "    #                         END OF YOUR CODE                           #\n",
    "    ######################################################################\n",
    "\n",
    "    # Save loss for later evaluation\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "    # Validation \n",
    "    # Placeholder variables for the predictions on the validation set\n",
    "    y_pred_val = None\n",
    "\n",
    "    ######################################################################\n",
    "    # TODO: Setup the validation process of an ANN.                      #          \n",
    "    # 1. Compute the value of the predictions y for the validation set.  #\n",
    "    # 2. Calculate the loss between the predictions and the true values. #\n",
    "    # 3. Save the loss for later evaluation.                             #\n",
    "    ######################################################################\n",
    "    \n",
    "    # Replace the pass statement with your code\n",
    "    pass\n",
    "\n",
    "    ######################################################################\n",
    "    #                         END OF YOUR CODE                           #\n",
    "    ######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e308f7e",
   "metadata": {
    "id": "8e308f7e"
   },
   "source": [
    " Afterwards, we plot the loss to see the training progress. The loss plot shows if adjustments need to be made to the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48549f90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "48549f90",
    "outputId": "1e941565-52e6-4459-b528-eb9bbd767538"
   },
   "outputs": [],
   "source": [
    "# Visualize the training process\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(train_loss, label='training')\n",
    "plt.plot(val_loss, label='validation', linestyle='--')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title('Loss plot')\n",
    "plt.show()\n",
    "\n",
    "# Printing the final value of the loss during training\n",
    "print('Final training loss: ')\n",
    "print(train_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90bbcc0",
   "metadata": {
    "id": "f90bbcc0"
   },
   "source": [
    "---\n",
    "Now you can play around with the hyperparameters *hidden_size* and *learning_rate* to get a feeling how they affect the prediction quality. The following cell summarizes the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7925e4c",
   "metadata": {},
   "source": [
    "*hidden_size* $\\in[1, 1024]$\n",
    "\n",
    "*learning_rate* $\\in(0, 1]$\n",
    "\n",
    "Please report the results which are printed below for comparison via this form: https://forms.office.com/e/s0Vjw05MHL. You can view the results in this sheet: https://forms.office.com/Pages/AnalysisPage.aspx?AnalyzerToken=PksOFfUOGgo8OKYPa4lQ65GzcefJ9gSX&id=TVJuCSlpMECM04q0LeCIexFyDp5Q34RIuGykwiiuPeZUNTBMU1BKOTRXTUZTNktOWFI1WTlJVFZOSiQlQCN0PWcu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb31dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "cbeb31dc",
    "outputId": "47e7c2c6-65b4-414d-a396-96e0e729b6e8"
   },
   "outputs": [],
   "source": [
    "# Summary of your NN layout and results\n",
    "print('Hidden layer size: ', hidden_size)\n",
    "print('Learning rate: ', learning_rate)\n",
    "print('Validation MSE: ', val_loss[-1]) # last element from the validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefddbc4",
   "metadata": {
    "id": "fefddbc4"
   },
   "source": [
    "## 1.5 - Evaluate ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700e451",
   "metadata": {
    "id": "0700e451"
   },
   "source": [
    "Now we evaluate the trained ANN by using it to make predictions on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45198ba5",
   "metadata": {
    "id": "45198ba5"
   },
   "outputs": [],
   "source": [
    "# Testing the network\n",
    "with torch.no_grad():\n",
    "    y_pred_test = net(torch.Tensor(x_test).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937822e",
   "metadata": {
    "id": "2937822e"
   },
   "source": [
    "For a first qualitative evaluation we plot both actual test data and the prediction by the ANN on the test set. We can play around with the hyperparameters and see how they affect the prediction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c42f00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "f4c42f00",
    "outputId": "cbbde5ec-1aff-421a-f1f8-faeec2d8ede5"
   },
   "outputs": [],
   "source": [
    "# Visualize the performance of the network\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(x, model(x), color='r', linestyle='--', label='ground truth')  # Ground truth from defined model\n",
    "plt.scatter(x_test, y_test, alpha=0.5, color='b', label='test data ground truth')  # Test points actual values\n",
    "plt.scatter(x_test, y_pred_test, alpha=0.5, color='r', label='test data prediction')  # Test points predicted values\n",
    "plt.title('ANN evaluation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e2070",
   "metadata": {
    "id": "012e2070"
   },
   "source": [
    "## 1.6 - Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d040a72",
   "metadata": {
    "id": "7d040a72"
   },
   "source": [
    "#### Quantitative assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e99d8",
   "metadata": {
    "id": "016e99d8"
   },
   "source": [
    "We were a little loose here by relying on a visual assessment of the fit. To systematically improve it we need a quantitative analysis of the errors. Therefore, we evaluate the mean squared error *MSE* of the test predictions.\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum_{i=1}^n{(Y_i-\\hat{Y}_i)^2}$\n",
    "\n",
    "For this we use the feed forward function of the ANN and get the prediction for the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344aff4b",
   "metadata": {
    "id": "344aff4b"
   },
   "outputs": [],
   "source": [
    "# Using the trained Neural Network for computing predictions\n",
    "with torch.no_grad():\n",
    "    y_pred_train = net(xt_train)\n",
    "    y_pred_val = net(xt_val)\n",
    "    y_pred_test = net(xt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc434e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    # Placeholder variable for the mean squared error\n",
    "    mse_loss = None\n",
    "    \n",
    "    ####################################################################\n",
    "    # TODO: Implement the mean squared error function.                 #\n",
    "    # Do not use any built-in PyTorch functions except torch.mean()    #\n",
    "    ####################################################################\n",
    "    \n",
    "    # Replace the pass statement with your code\n",
    "    pass\n",
    "    \n",
    "    ####################################################################\n",
    "    #                         END OF YOUR CODE                         #\n",
    "    ####################################################################\n",
    "\n",
    "    # The .item() method converts single element tensors to Python scalers for printing.\n",
    "    return mse_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb4a39",
   "metadata": {
    "id": "77fb4a39"
   },
   "outputs": [],
   "source": [
    "print('Training MSE: ', mse(yt_train, y_pred_train))\n",
    "print('Validation MSE: ', mse(yt_val, y_pred_val))\n",
    "print('Test MSE: ', mse(yt_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca5c7f",
   "metadata": {
    "id": "77ca5c7f"
   },
   "source": [
    "Another way to evaluate the model accuracy is to use a parity graph. In a parity graph, the ground truth is compared to the prediction. Ideally, the points should lay on the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f38afe",
   "metadata": {
    "id": "72f38afe"
   },
   "outputs": [],
   "source": [
    "v_min = np.min(y)\n",
    "v_max = np.max(y)\n",
    "plt.plot([v_min, v_max], [v_min, v_max], c=\"b\",label='Ideal Fit')\n",
    "\n",
    "################################################################################\n",
    "# TODO: Plot the parity graph.                                                 #\n",
    "# 1. Add a scatter plot of the ground truth vs. the predictions to the plot.   #\n",
    "################################################################################\n",
    "\n",
    "# Replace the pass statement with your code\n",
    "pass\n",
    "\n",
    "################################################################################\n",
    "#                             END OF YOUR CODE                                 #\n",
    "################################################################################\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('ground truth')\n",
    "plt.ylabel('prediction')\n",
    "plt.title('Parity graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab90451",
   "metadata": {
    "id": "8ab90451"
   },
   "source": [
    "#### Interpolation vs. Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c03904",
   "metadata": {
    "id": "94c03904"
   },
   "source": [
    "One important point to keep in mind is that purely data driven models cannot extrapolate. We see this in the following example. We increase the data range by 50% in both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846d327",
   "metadata": {
    "id": "7846d327"
   },
   "outputs": [],
   "source": [
    "# Creating the extrapolation dataset +/- 50% original range\n",
    "x_ext = np.linspace(np.min(x)-abs(x[-1]-x[0])/2, np.max(x)+abs(x[-1]-x[0])/2, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea568799",
   "metadata": {
    "id": "ea568799"
   },
   "source": [
    "Again, in order to work with PyTorch we transform our array to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12269a2",
   "metadata": {
    "id": "b12269a2"
   },
   "outputs": [],
   "source": [
    "xt_ext = torch.tensor(x_ext, dtype=torch.float).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5f72e",
   "metadata": {
    "id": "1dc5f72e"
   },
   "source": [
    "Now we use the trained model to predict values outside the training range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305ff4a",
   "metadata": {
    "id": "a305ff4a"
   },
   "outputs": [],
   "source": [
    "# neural network evaluation of the extrapolated range\n",
    "with torch.no_grad():\n",
    "    y_pred_ext = net(xt_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44894ce",
   "metadata": {
    "id": "f44894ce"
   },
   "source": [
    "We plot the ANN regression. The vertical bars mark the border of the training range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c896d4",
   "metadata": {
    "id": "45c896d4"
   },
   "outputs": [],
   "source": [
    "# Creating plot for comparison of the extrapolation capabilities\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.text(np.mean(x)-0.1*abs((np.max(x)-np.min(x))), np.min(y)+0.1*abs((np.max(y)-np.min(y))), 'interpolation', bbox=dict(facecolor = 'w'))\n",
    "plt.text(np.min(x)-0.8*abs((np.min(x_ext)-np.min(x))), np.min(y)+0.1*abs((np.max(y)-np.min(y))), 'extrapolation', bbox=dict(facecolor = 'w'))\n",
    "plt.text(np.max(x)+0.3*abs((np.max(x_ext)-np.max(x))), np.min(y)+0.1*abs((np.max(y)-np.min(y))), 'extrapolation', bbox=dict(facecolor = 'w'))\n",
    "plt.plot(x_ext, y_pred_ext, color='k', label='ANN prediction')\n",
    "plt.plot(x_ext, model(x_ext), color='r', label='ground truth', linestyle='--')\n",
    "plt.scatter(x_train, y_train, alpha=0.5, color='b', label='training data')\n",
    "plt.axvline(np.min(x), linestyle='--')\n",
    "plt.axvline(np.max(x), linestyle='--')\n",
    "plt.title('Regression Analysis')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed1b30",
   "metadata": {
    "id": "e8ed1b30"
   },
   "source": [
    "In the plot we clearly see that the neural network is not able to extrapolate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c49d4d",
   "metadata": {
    "id": "59c49d4d"
   },
   "source": [
    "## 1.7 - Conclusion\n",
    "\n",
    "In the first exercise of the lab we demonstrated how to use an ANN for regression. We introduced the key parameters to train an ANN and experienced, how they affect the training process. We also discussed extrapolation as a shortcoming of ANNs. Maybe you also experienced overfitting during your hyperparameter tuning?\n",
    "\n",
    "We hope you enjoyed it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067cf6a",
   "metadata": {
    "id": "2067cf6a"
   },
   "source": [
    "For solving the first part, the contents were covered during the first half of Lecture 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd363b1c",
   "metadata": {
    "id": "bd363b1c"
   },
   "source": [
    "## 2 - QSPR employing Neural Networks\n",
    "\n",
    "In the tutorial overview, you were introduced to \"Quatitative Structure-Property Relationships\" (QSPR). You might recall that this method is based on the defining a set of descriptors that depend on the structure of the molecule, followed by a regression model that predicts a property given a set of descriptors. Previously, we used a Linear Regression approach to define a model that aimed to predict the boiling point ($T_b$) [in K] of refrigerants. In this exercise, you are asked to generate a model - based on a feedforward ANN - to provide the boiling point of a refrigerant, given a set of descriptors ($D$).\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8326ac8",
   "metadata": {
    "id": "e8326ac8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qinghegao\\AppData\\Local\\Temp\\ipykernel_15020\\3968553629.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Standardizes features to a mean of 0 and variance of 1.\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6450fb9",
   "metadata": {
    "id": "b6450fb9"
   },
   "source": [
    "### 2.1 - Load the dataset\n",
    "\n",
    "Let's start by loading the dataset containing the descriptors and boiling points for 192 compounds.\n",
    "\n",
    "**Keep in mind that:** The table contains several refrigerants with different descriptors and their normal boiling point obtained from experiments.\n",
    "\n",
    "* The descriptors are the following:\n",
    "\n",
    "| Molecular Descriptor | Descriptor type | Descriptor definition |\n",
    "| --- | --- | --- |\n",
    "| R1e+ | GETAWAY descriptors | R maximal autocorrelation of lag 1/weighted by atomic Sanderson electronegativities |\n",
    "| MATS1m | 2D autocorrelation indices | Moran autocorrelation - lag 1/weighted by atomic masses |\n",
    "| X1sol | Connectivity indices | Solvation connectivity index chi-1 |\n",
    "| Me | Constitutional descriptors | Mean atomic Sanderson electronegativity (scaled on Carbon atom) |\n",
    "| ESpm02d | Edge adjacency indices | Spectral moment 02 from edge adj. matrix weighted by dipole moments |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a050916",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "2a050916",
    "outputId": "178bf50d-9b35-4651-ccd7-9f85a470ae68"
   },
   "outputs": [],
   "source": [
    "datafile_path = './data/boiling_point_data.csv'\n",
    "\n",
    "# Placeholder variables\n",
    "data = None\n",
    "\n",
    "###########################################################################################\n",
    "# TODO: Loading the dataset                                                               #\n",
    "# 1. Use the pandas library to load the data from the 'data/boiling_point_data.csv' file. #  \n",
    "###########################################################################################\n",
    "\n",
    "# Replace the pass statement with your code\n",
    "pass\n",
    "\n",
    "###########################################################################################\n",
    "#                             END OF YOUR CODE                                            #\n",
    "###########################################################################################\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d634635c",
   "metadata": {
    "id": "d634635c"
   },
   "source": [
    "### 2.2 - Data processing\n",
    "\n",
    "**Note:** To ensure reproducibility of our results, set the seed to 2024.\n",
    "\n",
    "In this part, your code should include:\n",
    "- $x_{train}$, $y_{train}$, $x_{val}$, $y_{val}$, $x_{test}$, and $y_{test}$: Use *sklearn.model_selection.train_test_split* function to get 80% training, 10% validation, and 10% test datast and set the random_state = 20.\n",
    "- Normalized $x_{train}$, $x_{val}$, and $x_{test}$- You can use the StandardScaler from the *sklearn* library\n",
    "- Create the tensors needed for training and testing the model(s) you will develop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0425ff",
   "metadata": {
    "id": "8c0425ff"
   },
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "torch.manual_seed(2024)\n",
    "np.random.seed(2024)\n",
    "\n",
    "# Extracting data\n",
    "x_all = data[['descriptor: R1e+','descriptor: MATS1m','descriptor: X1sol','descriptor: Me','descriptor: ESpm02d']].values\n",
    "y_all = data[['experiment: Tboil /K']].values\n",
    "\n",
    "#initialize the normalization\n",
    "st = StandardScaler()\n",
    "\n",
    "#########################################################################################################################\n",
    "# TODO: Data splitting and normalization                                                                                #\n",
    "# Helpful Link:                                                                                                         #\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html                       #\n",
    "# 1. Split the data into training set (80%), validation set (10%) and test set (10%), set the random state to 20.       #\n",
    "# 2. Create methods for normalizing the inputs (x) and outputs (y) of a model using the                                 #\n",
    "# MaxAbsScaler from the scikit-learn library.                                                                           #\n",
    "# 3. Transform the datasets using norm_entries for x_train, x_val and x_test.                                           #\n",
    "# 4. Create the tensors for the training and test datasets using the torch library.                                     #\n",
    "# Do not squeeze or unsqueeze the tensors.                                                                              #\n",
    "#########################################################################################################################\n",
    "\n",
    "# Replace the pass statement with your code\n",
    "pass\n",
    "\n",
    "###########################################################################################\n",
    "#                                  END OF YOUR CODE                                       #\n",
    "###########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a3ec2",
   "metadata": {
    "id": "b17a3ec2"
   },
   "source": [
    "### 2.3 - Model builiding and training\n",
    "\n",
    "Now with the tensors being properly defined, we can build and train the FFNN for predicting the boiling point of the molecule. In a similar fashion as in the previous exercises, you can tune the number of nodes per layer, the number of hidden layers, and the learning rate. In order to assess the accuracy of your model, calculate the MSE for the test set.\n",
    "\n",
    "In this task, your script should return:\n",
    "\n",
    "* `n_input`, `n_output`, `n_nodes`, `n_hidden_layers`, and `epochs` used for your FF-NN.\n",
    "* `loss_train` (Array): Containing the MSE of the training set per epoch.\n",
    "* `loss_test` (Array): Containing the MSE of the testing set per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c1e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# TODO: Define the architecture of the neural network.                          #\n",
    "# 1. Define the architecture of the neural network using the nn.Sequential()    #\n",
    "# method.                                                                       #\n",
    "# 2. Use the following layers:                                                  #\n",
    "#            One linear input layer + One Activation function (ReLU)            #\n",
    "#            4 (linear hidden layer + Activation function (ReLU))               #\n",
    "#            One linear output layer                                            #\n",
    "# Feel free to add more hidden layers with activation functions.                #\n",
    "# 3. Define the forward method to perform the feed forward pass.                #\n",
    "#################################################################################\n",
    "\n",
    "# Replace the pass statement with your code\n",
    "pass\n",
    "\n",
    "#################################################################################\n",
    "#                             END OF YOUR CODE                                  #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56601fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 5 # number of entries\n",
    "n_output = 1 # number of outputs\n",
    "n_nodes = 50 # number of neurons in the hidden layer\n",
    "learning_rate = 5e-4 # learning rate for the backpropagation\n",
    "\n",
    "# Neural network Definition\n",
    "net = NeuralNetwork(n_input, n_nodes, n_output) # create instance of neural network\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate) # sets optimizer and learning rate\n",
    "loss_fun = nn.MSELoss() # define loss function\n",
    "epochs = 600 # set number of epochs\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93a9ee",
   "metadata": {
    "id": "dc93a9ee"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "loss_train = np.empty(epochs)\n",
    "loss_test = np.empty(epochs)\n",
    "\n",
    "#################################################################################\n",
    "# TODO: Define the training process of the ANN.                                 #\n",
    "# Use the normalised datasets for training and testing.                         #\n",
    "# Define the training loop the same way as in section 1.                        #\n",
    "# For every epoch:                                                              #\n",
    "# 1. Clear the gradients for the next training epoch.                           #\n",
    "# 2. Compute the value of the predictions y for the training set.               #\n",
    "# 3. Calculate the loss between the predictions and the true values.            #\n",
    "# 4. Backward pass, compute gradients.                                          #\n",
    "# 5. Apply gradients to update weights and biases.                              #\n",
    "# 6. Save the training loss for later evaluation.                               #\n",
    "# 7. Compute the value of the predictions y for the test set.                   #\n",
    "# 8. Calculate the loss between the predictions and the true values.            #\n",
    "# 9. Save the test loss for later evaluation.                                   #\n",
    "#################################################################################\n",
    "\n",
    "# Replace the pass statement with your code\n",
    "pass\n",
    "\n",
    "#################################################################################\n",
    "#                         END OF YOUR CODE                                      #\n",
    "#################################################################################\n",
    "\n",
    "print('Final training loss: ')\n",
    "print(train_loss[-1])\n",
    "print('Results from your QSPR model using an ANN with: ')\n",
    "print('Hidden {:d} layers'.format(n_nodes))\n",
    "print('Learning rate: {:.3e}'.format(learning_rate))\n",
    "print('Training MSE: {:.3e}'.format(train_loss[-1])) # last element from the training loss\n",
    "print('Validation MSE: {:.3e}'.format(val_loss[-1])) # last element from the validation loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a421fd5",
   "metadata": {
    "id": "2a421fd5"
   },
   "source": [
    "### 2.4 - Visualizing the results\n",
    "\n",
    "With the previous arrays containing the loss for both, training and testing phases, we can evaluate how the network fits the (normalized) boiling point to the (normalized) descriptors. Your final visualization should resemble the trends shown in the provided 'Loss results' graph.\n",
    "\n",
    "In this step,\n",
    "\n",
    "* Plot the RSME values for the training set and test set predictions against the epochs.\n",
    "* Use a logarithmic scale for the loss axis.  \n",
    "* Label your axes properly and provide a legend to distinguish between the training and testing RSME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676426f2",
   "metadata": {
    "id": "676426f2"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# TODO: Create the loss plot for the training of your QSPR model                #\n",
    "# Plot the loss for the training and test set.                                  #\n",
    "# Use a logarithmic scale for the y-axis.                                       #\n",
    "#################################################################################\n",
    "\n",
    "# Replace the pass statement with your code\n",
    "pass\n",
    "\n",
    "#################################################################################\n",
    "#                         END OF YOUR CODE                                      #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede60b12",
   "metadata": {},
   "source": [
    "## 2.5 Prediction on test dataset\n",
    "Now that we have identified our optimal model, it's time to proceed with predictions on the test dataset. We will use the previously determined best degree and model for this purpose. Additionally, we will calculate the coefficient of determination $R^2$ and create a parity plot to illustrate the model's final performance.\n",
    "\n",
    "In this task,\n",
    "\n",
    "- $y_{predict}$: Utilize the *best_degree* and *best_model* you obtained before to get the final prediction on test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64095771",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################################################################\n",
    "# TODO: Model evaluation                                                         #\n",
    "# Helpful Tutorial: https://data36.com/polynomial-regression-python-scikit-learn/#\n",
    "# 1. utilize the trained net to make prediction on test dataset                  #\n",
    "# 2. Transform the test dataset into numpy format                                #\n",
    "##################################################################################\n",
    "\n",
    "# Replace the pass statement with your code\n",
    "pass\n",
    "\n",
    "##################################################################################\n",
    "#                              END OF YOUR CODE                                  #\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff273e0d",
   "metadata": {},
   "source": [
    "A parity plot is a type of scatter plot commonly used in regression analysis to visually assess the accuracy of a predictive model. It plots the actual values of the target variable against the predicted values obtained from the model. It provides a quick visual assessment of model performance across the range of observed values.\n",
    "\n",
    "Ideally, if a model predicts perfectly, all points in a parity plot would lie on a diagonal line indicating perfect agreement between actual and predicted values. This line is often referred to as the \"line of perfect fit\" or \"ideal fit\" line.\n",
    "\n",
    "In this task,\n",
    "\n",
    "- $r2$: Utilize the *r2_score* to calculate the coefficient of determination.\n",
    "- $RMSE$: Utilize the *root_mean_squared_error* to calculate the RMSE\n",
    "- parity polt: Include a scatter plot of actual values against predicted values, and an actual fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfce2f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# TODO: Basic Plotting and evaluation                                           #\n",
    "# Helpful Tutorial: https://matplotlib.org/stable/users/explain/quick_start.html#\n",
    "# 1. Calculate and print the R2 score and RMSE for the test data                #\n",
    "# 2. Make the parity plot using actual boiling points for the x-axis and        #\n",
    "#    predicted boiling points for the y-axis.                                   #\n",
    "# 3. Add a line through the plot to represent the ideal fit.                    #\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "# Replace the pass statement with your code\n",
    "pass\n",
    "\n",
    "#################################################################################\n",
    "#                         END OF YOUR CODE                                      #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa8d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
