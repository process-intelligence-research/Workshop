{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1 - Illustrative case study**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with artificial neural networks in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artificial neural network (ANN) is an universal function approximator. Contrary to curve-fitting regression, ANNs have the advantage that no a priori knowledge of the process is needed. Here we show an example of a simple ANN to approximate its target distribution. For the demonstration we use the programming language Python since it is the most popular machine learning language and offers a large and active developer community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opposite to matlab, we do not have all functionalities pre installed in Python. Therefore, the first step for a project is always to install packages which extend our function library. Here, we install the following packages:\n",
    "- *torch*: The PyTorch package is the most important one for this lab and inherits a variety of useful functions for machine learning.\n",
    "- *matplotlib*: Provides plotting functions similar to matlab.\n",
    "- *tqdm*: Allows to print progress bar of for loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use [PyTorch](https://pytorch.org/) to work with neural networks. Pytorch is an open source machine learning framework with many predefined functions which make the work with machine learning way easier. You can learn pytorch from the [tutorial link](https://pytorch.org/tutorials/). The Docs information can be searched at [Docs](https://pytorch.org/docs/stable/index.html). A popular alternative to Pytorch is Keras (building on Tensorflow). *optim* is a library of optimizers for the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install numpy torch torchvision torchaudio\n",
    "!pip install matplotlib\n",
    "!pip install tqdm\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Python preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we import the previously installed packages into our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # numerical calculations in python\n",
    "import matplotlib.pyplot as plt  # plotting similar to matlab\n",
    "import torch  # PyTorch: the general machine learning framework in Python\n",
    "import torch.optim as optim  # contains optimizers for the backpropagation\n",
    "import torch.nn as nn  # the artificial neural network module in PyTorch\n",
    "from tqdm import tqdm  # produces progress bars for for-loops in Python\n",
    "from sklearn.model_selection import train_test_split  # randomly splits a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our results reproducable, we need to set a so-called \"seed\". Machine Learning includes stochastic processes in the weight/bias initialization and the backpropagation. Also the random number generation which we will use for the dataset is a stochastic process. By setting a seed in the program we make sure that always the same random numbers are chosen. Otherwise, we would get different results everytime we run this script, which is not nice for teaching purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Prepare datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, we need a dataset to work on. Here, we simply make up our dataset from a self defined model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the model which we want to approximate. We add the option to add some noise to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model definition\n",
    "def model(x, noise=False):\n",
    "    y = np.sin(x)+np.sin(10/3*x) # model function\n",
    "    if noise:\n",
    "        y += 0.3*(np.random.uniform(-1, 1, x.size)) # add noise if noise=True\n",
    "    return y # return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our model to generate a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n = 100 # number of data points\n",
    "xmin = 2 # minimum value\n",
    "xmax = 6 # maximum value\n",
    "x = np.linspace(xmin, xmax, n) # generate equally spaced input values\n",
    "y = model(x, noise=True) # get output from our model with noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understanding of the data set we plot it with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4)) # define figure size\n",
    "plt.plot(x, model(x), color='r', linestyle='--', label='ground truth')\n",
    "plt.scatter(x, y, alpha=0.5, color='b', label='complete data') # scattered plot\n",
    "plt.title('Initial dataset') # add plot title\n",
    "plt.xlabel('x') # add x axis label\n",
    "plt.ylabel('y') # add y axis label\n",
    "plt.legend() # add plot legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split the dataset in training, validation and test data. In the first step we will split the data in training and remaining dataset. Now since we want the valation and test size to be equal (10% each of overall data), we have to define *train_size=0.5* (that is 50% of remaining data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# first split\n",
    "x_train, x_rem, y_train, y_rem = train_test_split(x, y, train_size=0.7)\n",
    "\n",
    "# second split\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_rem, y_rem, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the dataset division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4)) # define figure size\n",
    "plt.plot(x, model(x), color='r', linestyle='--', label='ground truth') # line plot\n",
    "plt.scatter(x_train, y_train, alpha=0.5, label='training data') # scattered plot\n",
    "plt.scatter(x_val, y_val, alpha=0.5, label='validation data') # scattered plot\n",
    "plt.scatter(x_test, y_test, alpha=0.5, label='test data') # scattered plot\n",
    "plt.title('Dataset division') # add plot title\n",
    "plt.xlabel('x') # add x axis label\n",
    "plt.ylabel('y') # add y axis label\n",
    "plt.legend() # add plot legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far our datasets are stored in numpy arrays. However, PyTorch works with tensors instead of arrays and we need to transform our data. [Here](https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c) you can find a blog post discussing the differences. We need to do two small technical changes as well. Numpy arrays usually use the double/float64 datatype whereas PyTorch uses float/float32. Therefore, we change the datatype *dtype*. In addition, we have to change the shape of the tensor from (#data) to (#data,1) with *unsqueeze*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xt_train = torch.tensor(x_train, dtype=torch.float).unsqueeze(-1)\n",
    "xt_val = torch.tensor(x_val, dtype=torch.float).unsqueeze(-1)\n",
    "xt_test = torch.tensor(x_test, dtype=torch.float).unsqueeze(-1)\n",
    "yt_train = torch.tensor(y_train, dtype=torch.float).unsqueeze(-1)\n",
    "yt_val = torch.tensor(y_val, dtype=torch.float).unsqueeze(-1)\n",
    "yt_test = torch.tensor(y_test, dtype=torch.float).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Set up ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the data we also nee to prepare our ANN. The *nn.Module* is the standard class for an ANN in PyTorch. The abbreviation *nn* stands for neural network. We build a child class of it where we specify our desired model architecture. Pytorch uses a base model object and adds the layers and activations as other objects in a sequential manner. The first layer must get an input dimensions matching the data, whereas the following can deduce their input size from the previous layer. The output layer then must match the dimension of the target values. Each ANN class needs a *forward* function which defines, how a signal propagates through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# neural network definition\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.architecture = nn.Sequential( \n",
    "            # sequential model definition: add up layers & activation functions\n",
    "            nn.Linear(in_features=n_input, out_features=n_hidden, bias=True),  # hidden layer\n",
    "            nn.Tanh(), # activation function\n",
    "            nn.Linear(in_features=n_hidden, out_features=n_hidden, bias=True),  # hidden layer\n",
    "            nn.Tanh(), # activation function\n",
    "            nn.Linear(in_features=n_hidden, out_features=n_output, bias=True)   # output layer\n",
    "        )\n",
    "    def forward(self, input): # feed forward path\n",
    "        output = self.architecture(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Train ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the fun begins. We put the dataset and ANN architecture together to \"train\" our ANN.\n",
    "\n",
    "We use the training data to train our neural network. This process is nothing else then optimizing the weights and biases in our network. Before starting the training process, we need to define a few things:\n",
    "- *optimizer*: Here we use *SGD* which stands for [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). With the optimizer, we also need to define the learning rate *lr*. It determines how fast we adopt the weights and biases during the training. If it is too high, the learning becomes instable and the loss increases. If it is too low, we need to many epochs and we do not reach a satisfying precision.\n",
    "- *loss function*: This is the objective of our training/optimization. For continuous outputs as in our example, you usually use the mean squared error. For discrete outputs, another function is needed, like the cross entropy.\n",
    "- *epochs*: How often do we want to repeat the training with our dataset?\n",
    "\n",
    "Optimizing these parameters is called hyerparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Adjust the following hyperparameters to find the optimal combination:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 20 # number of neurons in the hidden layer\n",
    "learning_rate = 0.09 # learning rate for the backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*hidden_size* $\\in[1, 1024]$\n",
    "\n",
    "*learning_rate* $\\in(0, 1]$\n",
    "\n",
    "Please report the results which are printed below (after the loss plot) for comparison via this form: https://forms.gle/vg4hJAmBSqgzspBf6. You can view the results in this sheet: https://docs.google.com/spreadsheets/d/1Rzf1-g-QX_2QRsPoFU0Xw1UYO5OKrO4ebMfN6KfJxS4/edit?usp=sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell prepares everything for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# neural network training\n",
    "net = NeuralNetwork(1, hidden_size, 1) # create instance of neural network\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate) # choose optimizer and learning rate\n",
    "loss_fun = nn.MSELoss() # define loss function\n",
    "epochs = 5000 # set number of epochs\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main part of using an ANN: the actual \"training\". We give an input to the network and see how the output differs from our expected output. The difference is used to calculate the loss. Then we update the weights and biases such that the loss will be smaller in the next epoch.\n",
    "\n",
    "In the training process we use two datasets: the **training** and the **validation** data. The training data are used to calculate the training loss, which is then used for the backpropagation and the network update. The validation data are used to detect overfitting. We just calculate the loss for the validation data, but do not use it for the backpropagation. If the training and the validation loss diverge, we know that the network updates do not generalize for unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# train the network\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    # training data\n",
    "    optimizer.zero_grad() # clear gradients for next training epoch\n",
    "    y_pred = net(xt_train) # forward pass: prediction y based on input x\n",
    "    loss = loss_fun(y_pred, yt_train)  # compare true y and predicted y to get the loss\n",
    "    loss.backward() # backpropagation, compute gradients\n",
    "    optimizer.step() # apply gradients to update weights and biases\n",
    "    train_loss.append(loss.item()) # save loss for later evaluation\n",
    "\n",
    "    # validation data\n",
    "    loss = loss_fun(y_pred, yt_train) # compare true y and predicted y to get the loss\n",
    "    val_loss.append(loss.item()) # save loss for later evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Afterwards, we plot the loss to see the training progress. The loss plot shows if adjustments need to be made to the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(train_loss, label='training')\n",
    "plt.plot(val_loss, label='validation', linestyle='--')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title('Loss plot')\n",
    "plt.show()\n",
    "print('Final training loss: ')\n",
    "print(train_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now you can play around with the hyperparameters *hidden_size* and *learning_rate* to get a feeling how they affect the prediction quality. The following cell summarizes the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hidden layer size: ', hidden_size)\n",
    "print('Learning rate: ', learning_rate)\n",
    "print('Validation MSE: ', val_loss[-1]) # last element from the validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - Evaluate ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the trained ANN by using it to predict our known data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network evaluation\n",
    "with torch.no_grad():\n",
    "    y_pred = net(torch.Tensor(x).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first qualitative evaluation we plot both actual test output data and the prediction by the ANN on the test set. We can play around with the hyperparameters and see how they affect the prediction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(x, y_pred, color='k', label='ANN prediction')\n",
    "plt.plot(x, model(x), color='r', linestyle='--', label='ground truth')\n",
    "plt.scatter(x_test, y_test, alpha=0.5, color='b', label='test data')\n",
    "plt.title('ANN evaluation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 - Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantitative assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were a little loose here by relying on a visual assessment of the fit. To systematically improve it we need a quantitative analysis of the errors. Therefore, we evaluate the mean squared error *MSE* of the test predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE = \\frac{1}{n}\\sum_{i=1}^n{(Y_i-\\hat{Y}_i)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we use the feed forward function of the ANN and get the prediction for the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "with torch.no_grad():\n",
    "    y_pred_train = net(xt_train)\n",
    "    y_pred_val = net(xt_val)\n",
    "    y_pred_test = net(xt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .item() method converts single element tensors to Python scalers for printing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(( yt_train - y_pred_train )**2).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(( yt_val - y_pred_val )**2).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(( yt_test - y_pred_test )**2).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to evaluate the model accuracy is to use a Pareto chart. In a Pareto chart, the ground truth is compared to the prediction. Ideally, the points should lay on the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train, y_pred_train, alpha=0.1, label=\"Training data\")\n",
    "plt.scatter(y_val, y_pred_val, alpha=0.3, label=\"Validation data\")\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5, label=\"Test data\")\n",
    "plt.legend()\n",
    "plt.xlabel('ground truth')\n",
    "plt.ylabel('prediction')\n",
    "min = np.min(y)\n",
    "max = np.max(y)\n",
    "plt.plot([min, max], [min, max], c=\"b\")\n",
    "plt.title('Pareto chart')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation vs. Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important point to keep in mind is that purely data driven models cannot extrapolate. We see this in the following example. We increase the data range by 50% in both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ext = np.linspace(np.min(x)-abs(x[-1]-x[0])/2, np.max(x)+abs(x[-1]-x[0])/2, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, in order to work with PyTorch we transform our array to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_ext = torch.tensor(x_ext, dtype=torch.float).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the trained model to predict values outside the training range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network evaluation\n",
    "with torch.no_grad():\n",
    "    y_pred_ext = net(xt_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the ANN regression. The vertical bars mark the border of the training range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.text(np.mean(x)-0.1*abs((np.max(x)-np.min(x))), np.min(y)+0.1*abs((np.max(y)-np.min(y))), 'interpolation', bbox=dict(facecolor = 'w'))\n",
    "plt.text(np.min(x)-0.8*abs((np.min(x_ext)-np.min(x))), np.min(y)+0.1*abs((np.max(y)-np.min(y))), 'extrapolation', bbox=dict(facecolor = 'w'))\n",
    "plt.text(np.max(x)+0.3*abs((np.max(x_ext)-np.max(x))), np.min(y)+0.1*abs((np.max(y)-np.min(y))), 'extrapolation', bbox=dict(facecolor = 'w'))\n",
    "plt.plot(x_ext, y_pred_ext, color='k', label='ANN prediction')\n",
    "plt.plot(x_ext, model(x_ext), color='r', label='ground truth', linestyle='--')\n",
    "plt.scatter(x_train, y_train, alpha=0.5, color='b', label='training data')\n",
    "plt.axvline(np.min(x), linestyle='--')\n",
    "plt.axvline(np.max(x), linestyle='--')\n",
    "plt.title('Regression Analysis')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot we clearly see that the neural network is not able to extrapolate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first half of the lab we demonstrated how to use an ANN for regression. We introduced the key parameters to train an ANN and experienced, how they affect the training process. We also discussed extrapolation as a shortcoming of ANNs. Maybe you also experienced overfitting during your hyperparameter tuning?\n",
    "\n",
    "We hope you enjoyed it!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a6378035587bb97055001603ea9d85a2aa377cc6252a50ffca4355a71bc8b90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
